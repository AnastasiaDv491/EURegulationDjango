{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "import os\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'RiskconcileData.settings')\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "django.setup()\n",
    "\n",
    "from RiskconcileData.db.models import Regulation, RegulationRelation\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "from py2neo import Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'bolt://localhost:7687'\n",
    "username = ''\n",
    "password = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = (username, password)\n",
    "graph = Graph(host, auth=auth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Nodes and attributes </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "        data: pd.DataFrame,\n",
    "        data_present: pd.DataFrame,\n",
    "        join_variable: Union[str, list]\n",
    "        ) -> tuple:\n",
    "    \"\"\" Splits the data on in three different datasets:\n",
    "       1. To create: contains the nodes that need to be created\n",
    "       2. To delete: contains the nodes that need to be deleted\n",
    "       3. To update: contains the nodes that need to be updated\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): the newly loaded data that needs to be \n",
    "            uploaded to Neo4J\n",
    "        data_present (pd.DataFrame): the data currently in Neo4j\n",
    "        join_variable (str): the variable on which the datasets are \n",
    "            joined. This usually corresponds to the key of the data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: a tuple containing the to_create, to_delete, and to_update\n",
    "            dataframes.\n",
    "\n",
    "    Example usage\n",
    "    -------------\n",
    "    to_create, to_delete, to_update = split_data(\n",
    "        data=relations,\n",
    "        data_present=existing_relations,\n",
    "        join_variable=[\"id\", \"industry\"]\n",
    "    )\n",
    "    \"\"\"\n",
    "    assert_message = \"Please provide data as a dataframe, \" \\\n",
    "            + \"if you provided a single \" \\\n",
    "            + \"column dataframe (i.e. a pandas series), be sure to use \" \\\n",
    "            + \"the Series.to_frame() method.\"\n",
    "    assert type(data) is pd.DataFrame, assert_message\n",
    "    assert type(data_present) is pd.DataFrame, assert_message\n",
    "\n",
    "    if data_present.empty:\n",
    "        to_create = data.copy()\n",
    "        to_delete = pd.DataFrame()\n",
    "        to_update = pd.DataFrame()\n",
    "    else:\n",
    "        if type(join_variable) is str:\n",
    "            assert join_variable in data.columns, \\\n",
    "                f\"'{join_variable}' not in data.\"\n",
    "            assert join_variable in data_present.columns, \\\n",
    "                f\"'{join_variable}' not in present data.\"\n",
    "        elif type(join_variable) is list:\n",
    "            for variable in join_variable:\n",
    "                assert variable in data.columns, \\\n",
    "                f\"'{variable}' not in data.\"\n",
    "                assert variable in data_present.columns, \\\n",
    "                    f\"'{variable}' not in present data.\"\n",
    "        temp = pd.merge(data, \n",
    "                data_present[join_variable], # Avoids the creation of variable_x, variable_y type of columns\n",
    "                on=join_variable,\n",
    "                how=\"outer\", \n",
    "                indicator=True)\n",
    "        to_create = temp[temp._merge == \"left_only\"].copy()\n",
    "        to_delete = temp[temp._merge == \"right_only\"].copy()\n",
    "        to_update = temp[temp._merge == \"both\"].copy()\n",
    "    return to_create, to_delete, to_update\n",
    "\n",
    "\n",
    "def create_nodes(\n",
    "        data: pd.DataFrame,\n",
    "        name: str,\n",
    "        attributes: list,\n",
    "        attribute_names: list,\n",
    "        graph: Graph) -> str:\n",
    "    \"\"\" Create new nodes with a certain name and attributes based on \n",
    "    data that can be found in a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): the dataframe containing all node information\n",
    "        name (str): the name of the node that needs to be created (Neo4J label)\n",
    "        attributes (list): the attributes as can be found in the database\n",
    "            columns\n",
    "        attribute_names (list): the attributes as you would like them\n",
    "            to appear in the neo4j database. Same order as attributes.\n",
    "        graph (Graph): the neo4j database to which you would like to\n",
    "            upload the data,\n",
    "\n",
    "    Returns:\n",
    "        str: simple success message\n",
    "    \"\"\"\n",
    "    assert all(elem in data.columns for elem in attributes), \\\n",
    "        \"Ensure that all attributes are columns in the dataframe\"\n",
    "    assert len(attributes) == len(attribute_names), \\\n",
    "        \"Ensure that the attributes and attribute names have the same length\"\n",
    "    attribute_string='{'\n",
    "    for i in range(len(attributes)):\n",
    "        if \"_date\" not in attribute_names[i]:\n",
    "            attribute_string += f'{attribute_names[i]}: row.{attributes[i]}'\n",
    "        else:\n",
    "            attribute_string += f'{attribute_names[i]}: date(row.{attributes[i]})'\n",
    "        if i < len(attributes) - 1:\n",
    "            attribute_string += ','\n",
    "    attribute_string += '})'\n",
    "    upload_query = f'''\n",
    "        UNWIND $rows AS row\n",
    "        MERGE (n:{name}\n",
    "        {attribute_string}\n",
    "    '''\n",
    "    graph.run(upload_query, parameters = {'rows': pd.DataFrame(data).to_dict('records')})\n",
    "    return 'SUCCESS'\n",
    "\n",
    "def delete_nodes(data: pd.DataFrame,\n",
    "            name: str,\n",
    "            attributes: list,\n",
    "            attribute_names: list,\n",
    "            graph: Graph) -> str:\n",
    "    \"\"\" Deletes the nodes that can be found in the dataframe\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): dataframe consisting of the observations\n",
    "            that need to be removed.\n",
    "        name (str): the name of the nodes\n",
    "        attributes (list): the attributes on which nodes are matched as\n",
    "            can be found in the dataframe\n",
    "        attribute_names (list): the attributes on which nodes are matched\n",
    "            as found in the graph database\n",
    "        graph (Graph): the graph database connection\n",
    "\n",
    "    Returns:\n",
    "        str: success indicator\n",
    "    \"\"\"\n",
    "    if not data.empty:\n",
    "        query = \"UNWIND $rows AS row MATCH (n:\"\n",
    "        query += name + '{'\n",
    "        query += attributes[0] + ': row.'\n",
    "        query += attribute_names[0] + '})'\n",
    "        query += \" detach delete n\"\n",
    "        graph.run(query,\n",
    "             parameters = {'rows': pd.DataFrame(data).to_dict(orient='records')})\n",
    "    return 'SUCCESS'\n",
    "\n",
    "\n",
    "def update_nodes(data: pd.DataFrame,\n",
    "                data_present: pd.DataFrame,\n",
    "                name: str,\n",
    "                matching_attribute: str,\n",
    "                matching_attribute_name: str,\n",
    "                attributes: list,\n",
    "                attribute_names: list,\n",
    "                graph: Graph) -> str:\n",
    "    \"\"\" This function updates the nodes that are present in both the \n",
    "    dataframe and the graph database. Only those nodes that actually\n",
    "    changed are updated.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataframe containing the most up-to-date\n",
    "        data\n",
    "        data_present (pd.DataFrame): the data currently in the graph \n",
    "        database\n",
    "        name (str): the name of the nodes that need to be updated\n",
    "        matching_attribute (str): the attributes on which nodes are\n",
    "        matched as found in the dataframe\n",
    "        matching_attribute_name (str): the attributes on which nodes are\n",
    "        matched as found in the graph database\n",
    "        attributes (list): the attributes that need to be updated as \n",
    "        found in the data\n",
    "        attribute_names (list): the attributes that need to be updated\n",
    "        as found in the graph database\n",
    "        graph (Graph): the graph database connection\n",
    "\n",
    "    Returns:\n",
    "        str: success indicator\n",
    "    \"\"\"\n",
    "    if not data_present.empty:\n",
    "        to_update = pd.merge(\n",
    "            data_present,\n",
    "            data,\n",
    "            on=attributes.copy().append(matching_attribute),\n",
    "            how='outer',\n",
    "            indicator=\"ind\"\n",
    "        )\n",
    "        changed_data = to_update[to_update.ind == \"right_only\"].copy()\n",
    "        sets = \"\"\n",
    "        for i in range(len(attributes)):\n",
    "            sets += f\"SET n.{attribute_names[i]} = row.{attributes[i]} \\n\"\n",
    "        query = f\"UNWIND $rows AS row MATCH (n:{name} \"\n",
    "        query += \"{\" + matching_attribute_name + \": row.\"\n",
    "        query += matching_attribute + \"})\"\n",
    "        query += sets\n",
    "        graph.run(query, parameters = {'rows': changed_data.to_dict('records')})\n",
    "    return \"SUCCESS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "today = date.today()\n",
    "week_prior =  today - timedelta(weeks=1)\n",
    "\n",
    "df_regulation = pd.DataFrame(Regulation.objects.all().values())\n",
    "df_relation = pd.DataFrame(RegulationRelation.objects.all().values())\n",
    "\n",
    "def merge_Reg_Rel_tables(reg_table, rel_table):\n",
    "\n",
    "    reg_table = reg_table.rename(columns={\"doc_code\": \"source_id\"})\n",
    "    # Merge using \"source\" column\n",
    "    merged_table =pd.merge(rel_table,reg_table, on = \"source_id\", how = 'left')\n",
    "    merged_table=merged_table.rename(columns={\"name\": \"name_src\", \"celex_code\": \"celex_code_src\",\"title\": \"title_src\", \"publication_date\": \"publication_date_src\", \"doc_type\":\"doc_type_src\",\"doc_type_code\": \"doc_type_src\", \"author\": \"author_src\", \"languages\":\"languages_src\", \"url\": \"url_src\"})\n",
    "\n",
    "    # Merge using \"target\" column\n",
    "    reg_table = reg_table.rename(columns={\"source_id\": \"target_id\"})\n",
    "    merged_table = merged_table.merge(reg_table, how = 'left', on = \"target_id\")\n",
    "    merged_table=merged_table.rename(columns={\"name\": \"name_trg\", \"celex_code\": \"celex_code_trg\",\"title\": \"title_trg\", \"publication_date\": \"publication_date_trg\", \"doc_type\":\"doc_type_trg\",\"doc_type_code\": \"doc_type_trg\", \"author\": \"author_trg\", \"languages\":\"languages_trg\", \"url\": \"url_trg\"})\n",
    "    merged_table = merged_table.fillna(value = \"Not found\")\n",
    "    merged_table = merged_table.drop_duplicates()\n",
    "\n",
    "    return merged_table\n",
    "\n",
    "merge_Reg_Rel_tables(df_regulation,df_relation )\n",
    "\n",
    "\n",
    "\n",
    "df_regulation_merged  = merge_Reg_Rel_tables(df_regulation,df_relation )\n",
    "\n",
    "def create_new_nodes(data: pd.DataFrame):\n",
    "    data[\"publication_date\"] = data[\"publication_date\"].astype(\"string\").str.extract('(\\d{2}\\/\\d{2}\\/\\d{4})').fillna('Not found')\n",
    "  \n",
    "    return data\n",
    "\n",
    "def create_author_nodes(data: pd.DataFrame):\n",
    "    df = data[[\"doc_code\", \"author\"]]\n",
    "    df[\"author\"] = df[\"author\"].astype(str).str.split(\", \").replace(\"'\", \"\")\n",
    "    df = df.explode(\"author\")\n",
    "    return df\n",
    "\n",
    "def create_lang_nodes(data: pd.DataFrame):\n",
    "    df = data[[\"doc_code\", \"languages\"]]\n",
    "    df[\"languages\"] = df[\"languages\"].astype(str).str.split(\", \").replace(\"'\", \"\")\n",
    "    df = df.explode(\"languages\")\n",
    "    return df\n",
    "\n",
    "def create_class_nodes(data: pd.DataFrame):\n",
    "    df = data[[\"doc_code\", \"classification\"]]\n",
    "    df[\"classification\"] = df[\"classification\"].replace(\"'\", \"\", regex = True).replace(\"\\[\", \"\", regex = True).replace(\"\\]\", \"\", regex = True)\n",
    "    df[\"classification\"] = df[\"classification\"].astype(str).str.split(\", \").replace(\"'\", \"\")\n",
    "\n",
    "    df = df.explode(\"classification\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_name: str = 'regulation'\n",
    "\n",
    "types = pd.Series(df_regulation.doc_code.unique(), name=\"doc_code\")\n",
    "types_present = graph.run(f'match (s:{node_name}) return s.doc_code as doc_code').to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_create, to_delete, to_update = split_data(\n",
    "    data=types.to_frame(),\n",
    "    data_present=types_present,\n",
    "    join_variable=\"doc_code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_nodes(df_regulation_merged, \"regulation\", [\"source_id\", \"celex_code_src\",\"title_src\", \"quicksearch_url_x\", \"date_effect_x\", \"date_sig_x\", \"date_deadline_x\"], [\"doc_code\",\"celex_code\", \"title\",  \"quicksearch_url\", \"date_effect\", \"date_signature\", \"deadlines\"], graph)\n",
    "create_nodes(df_regulation_merged, \"regulation\", [\"target_id\", \"celex_code_trg\",\"title_trg\", \"quicksearch_url_y\", \"date_effect_y\", \"date_sig_y\", \"date_deadline_y\"], [ \"doc_code\",\"celex_code\",\"title\",  \"quicksearch_url\", \"date_effect\", \"date_signature\", \"deadlines\"], graph)\n",
    "create_nodes(create_new_nodes(df_regulation), \"date\", [\"publication_date\"], [\"date\"], graph)\n",
    "create_nodes(create_author_nodes(df_regulation), \"author\", [\"author\"], [\"author\"], graph)\n",
    "create_nodes(create_lang_nodes(df_regulation), \"languages\", [\"languages\"], [\"languages\"], graph)\n",
    "create_nodes(create_class_nodes(df_regulation), \"classification\", [\"classification\"], [\"classification\"], graph)\n",
    "create_nodes(df_regulation, \"type\", [\"doc_type\"], [\"doc_type\"], graph)\n",
    "create_nodes(df_regulation, \"name\", [\"name\"], [\"name\"], graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Relations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relations(data: pd.DataFrame,\n",
    "                    source_node_name: str,\n",
    "                    dest_node_name: str,\n",
    "                    source_node_attribute: str,\n",
    "                    dest_node_attribute: str,\n",
    "                    source_node_attribute_name: str,\n",
    "                    dest_node_attribute_name: str,\n",
    "                    relation_name: str,\n",
    "                    graph: Graph,\n",
    "                    relation_attributes: list=None,\n",
    "                    relation_attribute_names: list=None) -> str:\n",
    "    \"\"\" This function creates relationships between nodes based on\n",
    "    data in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): the data that needs to be created\n",
    "        source_node_name (str): the name of the source node\n",
    "        dest_node_name (str): the name of the destination node\n",
    "        source_node_attribute (str): the attributes of the source node\n",
    "            used for matching as they appear in the data\n",
    "        dest_node_attribute (str): the attributes of the target node\n",
    "            used for matching as they appear in the data\n",
    "        source_node_attribute_name (str): the attributes of the source node\n",
    "            used for matching as they appear in the graph database\n",
    "        dest_node_attribute_name (str): the attributes of the target node\n",
    "            used for matching as they appear in the graph database\n",
    "        relation_name (str): the name of the relation\n",
    "        graph (Graph): graph database\n",
    "        relation_attributes (list, optional): the attributes for the \n",
    "            relation as they appead in the data. Defaults to None.\n",
    "        relation_attribute_names (list, optional): the attributes for the \n",
    "            relation as they appead in the graph database. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: success indicator\n",
    "    \"\"\"\n",
    "    attribute_string = ''\n",
    "    if relation_attributes is not None \\\n",
    "            and relation_attribute_names is not None:\n",
    "        assert len(relation_attributes) == len(relation_attribute_names), \\\n",
    "            \"Ensure that the relation attributes and relation attribute names have the same length\"\n",
    "        attribute_string='{'\n",
    "        for i in range(len(relation_attributes)):\n",
    "            # Dates need to be explicitly created as dates in neo4j. Since in \n",
    "            # our data, a date is always found in a column that contains the \n",
    "            # string _date in its name, we can handle them using a condition\n",
    "            if \"_date\" not in relation_attribute_names[i]:\n",
    "                attribute_string += f'{relation_attribute_names[i]}: row.{relation_attributes[i]}'\n",
    "            else:\n",
    "                attribute_string += f'{relation_attribute_names[i]}: date(row.{relation_attributes[i]})'\n",
    "            if i < len(relation_attributes) - 1:\n",
    "                attribute_string += ','\n",
    "        attribute_string += '}'\n",
    "    query = \"UNWIND $rows AS row \\n\"\n",
    "    query += f\"MATCH (n1:{source_node_name} \" + '{'\n",
    "    query += source_node_attribute_name + f': row.{source_node_attribute}' + '}), '\n",
    "    query += f\"(n2:{dest_node_name} \" + '{'\n",
    "    query += dest_node_attribute_name + f': row.{dest_node_attribute}' + '}) \\n'\n",
    "    query += f\"CREATE (n1) -[rel:{relation_name} {attribute_string}]-> (n2) \\n RETURN count(*) as total \"\n",
    "    graph.run(query, parameters = {'rows': data.to_dict('records')})\n",
    "    return \"SUCCESS\"\n",
    "\n",
    "\n",
    "def delete_relations(data: pd.DataFrame,\n",
    "                    source_node_name: str,\n",
    "                    dest_node_name: str,\n",
    "                    source_node_attribute: str,\n",
    "                    dest_node_attribute: str,\n",
    "                    source_node_attribute_name: str,\n",
    "                    dest_node_attribute_name: str,\n",
    "                    relation_name: str,\n",
    "                    graph: Graph):\n",
    "    if not data.empty:\n",
    "        data = data[[source_node_attribute, dest_node_attribute]]\n",
    "        query = \"UNWIND $rows AS row \\n\"\n",
    "        query += f\"MATCH (n1:{source_node_name} \" + \"{\" + source_node_attribute_name + f\": row.{source_node_attribute}\" + \"}) \"\n",
    "        query += f\"-[rel:{relation_name}]-> (n2:{dest_node_name}\" + \"{\"\n",
    "        query += dest_node_attribute_name + \": row.\" + dest_node_attribute + \"}) \\n\"\n",
    "        query += \"DELETE rel\"\n",
    "        graph.run(query,\n",
    "                 parameters = {'rows': data.to_dict(orient='records')})\n",
    "    return 'SUCCESS'\n",
    "\n",
    "\n",
    "def update_relations(relations: pd.DataFrame,\n",
    "                relations_present: pd.DataFrame,\n",
    "                name: str,\n",
    "                source_name: str,\n",
    "                target_name: str,\n",
    "                matching_attribute_source: str,\n",
    "                matching_attribute_source_name: str,\n",
    "                matching_attribute_target: str,\n",
    "                matching_attribute_target_name: str,\n",
    "                attributes: list,\n",
    "                attribute_names: list,\n",
    "                graph: Graph):\n",
    "    if not relations_present.empty:\n",
    "        to_update = pd.merge(\n",
    "            relations_present,\n",
    "            relations,\n",
    "            on=attributes.copy() + [matching_attribute_source_name, matching_attribute_target_name],\n",
    "            how='outer',\n",
    "            indicator=\"ind\"\n",
    "        )\n",
    "        changed_data = to_update[to_update.ind == \"right_only\"].copy()\n",
    "        sets = \"\"\n",
    "        for i in range(len(attributes)):\n",
    "            # Dates need to be explicitly created as dates in neo4j. Since in \n",
    "            # our data, a date is always found in a column that contains the \n",
    "            # string _date in its name, we can handle them using a condition\n",
    "            if \"_date\" not in attribute_names[i]:\n",
    "                sets += f\"SET r.{attribute_names[i]} = row.{attributes[i]} \\n\"\n",
    "            else:\n",
    "                sets += f\"SET r.{attribute_names[i]} = date(row.{attributes[i]}) \\n\"\n",
    "        query = f\"UNWIND $rows AS row MATCH (e1:{source_name} \\n\"\n",
    "        query += \"{\" + matching_attribute_source_name + \": row.\"\n",
    "        query += matching_attribute_source + \"}) \"\n",
    "        query += f\"-[r:{name}]-> (e2:{target_name} \"\n",
    "        query += \"{\" + matching_attribute_target_name + \": row.\"\n",
    "        query += matching_attribute_target + \"}) \\n\"\n",
    "        query += sets\n",
    "        graph.run(query, parameters = {'rows': changed_data.to_dict('records')})\n",
    "    return \"SUCCESS\"\n",
    "\n",
    "\n",
    "def delete_relations_id(data: pd.DataFrame,\n",
    "                        name: str,\n",
    "                        id_column: str,\n",
    "                        id_attribute_name: str,\n",
    "                        graph: Graph):\n",
    "    query = f\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MATCH (e1) -[r:{name}]- (e2)\n",
    "        WHERE r.{id_attribute_name} = row.{id_column}\n",
    "        DELETE r\n",
    "    \"\"\"\n",
    "    graph.run(query, parameters = {'rows': data.to_dict(orient='records')})\n",
    "    return 'SUCCESS'\n",
    "\n",
    "\n",
    "def update_relations_id(relations: pd.DataFrame,\n",
    "                        relations_present: pd.DataFrame,\n",
    "                        name: str,\n",
    "                        id_column: str,\n",
    "                        id_attribute_name: str,\n",
    "                        attributes: list,\n",
    "                        attribute_names: list,\n",
    "                        graph: Graph):\n",
    "    if not relations_present.empty:\n",
    "        to_update = pd.merge(\n",
    "            relations_present,\n",
    "            relations,\n",
    "            on=attributes.copy() + [id_column],\n",
    "            how='outer',\n",
    "            indicator=\"ind\"\n",
    "        )\n",
    "        changed_data = to_update[to_update.ind == \"right_only\"].copy()\n",
    "        sets = \"\"\n",
    "        for i in range(len(attributes)):\n",
    "            # Dates need to be explicitly created as dates in neo4j. Since in \n",
    "            # our data, a date is always found in a column that contains the \n",
    "            # string _date in its name, we can handle them using a condition\n",
    "            if \"_date\" not in attribute_names[i]:\n",
    "                sets += f\"SET r.{attribute_names[i]} = row.{attributes[i]} \\n\"\n",
    "            else:\n",
    "                sets += f\"SET r.{attribute_names[i]} = date(row.{attributes[i]}) \\n\"\n",
    "        query = f\"\"\"\n",
    "            UNWIND $rows AS row\n",
    "            MATCH (e1) -[r:{name}]- (e2)\n",
    "            WHERE r.{id_attribute_name} = row.{id_column}\n",
    "        \"\"\"\n",
    "        query += sets\n",
    "        graph.run(query, parameters = {'rows': changed_data.to_dict('records')})\n",
    "    return \"SUCCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amending = df_relation[df_relation[\"relation_type\"]== \"amending\"]\n",
    "df_supplementing = df_relation[df_relation[\"relation_type\"]== \"supplementing\"]\n",
    "df_referencing = df_relation[df_relation[\"relation_type\"]== \"references\"]\n",
    "\n",
    "create_relations(df_amending, \"regulation\", \"regulation\",\"source_id\", \"target_id\", \"doc_code\", \"doc_code\", \"AMENDING\", graph)\n",
    "create_relations(df_supplementing, \"regulation\", \"regulation\",\"source_id\", \"target_id\",\"doc_code\", \"doc_code\",\"SUPPLEMENTING\", graph)\n",
    "create_relations(df_referencing, \"regulation\", \"regulation\", \"source_id\", \"target_id\",\"doc_code\", \"doc_code\",\"REFERENCES\", graph)\n",
    "create_relations(df_regulation,\"regulation\", \"date\", \"doc_code\", \"publication_date\",\"doc_code\", \"date\", \"PUBLISHED\", graph )\n",
    "create_relations(df_regulation,\"regulation\", \"author\", \"doc_code\", \"author\",\"doc_code\", \"author\", \"AUTHOR\", graph )\n",
    "create_relations(df_regulation,\"regulation\", \"languages\", \"doc_code\", \"languages\",\"doc_code\", \"languages\", \"language\", graph )\n",
    "create_relations(df_regulation,\"regulation\", \"classification\", \"doc_code\", \"classification\",\"doc_code\", \"classfification\", \"CATEGORY\", graph )\n",
    "create_relations(df_regulation,\"regulation\", \"author\", \"doc_code\", \"author\",\"doc_code\", \"author\", \"AUTHOR\", graph )\n",
    "create_relations(df_regulation,\"regulation\", \"type\", \"doc_code\", \"doc_type\",\"doc_code\", \"doc_type\", \"TYPE\", graph )\n",
    "create_relations(df_regulation,\"regulation\", \"name\", \"doc_code\", \"name\",\"doc_code\", \"name\", \"NAME\", graph )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
